{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "import os, re\n",
    "import string\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk import wordpunct_tokenize\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "def retrieve_readme(url1,suffix):\n",
    "    r = requests.get(url1+'/readme')\n",
    "    t = r.text\n",
    "    dic = json.loads(t)\n",
    "#    print(str(base64.b64decode(dic['content']).decode('UTF8')))\n",
    "    with open('./Readme/readme'+suffix+'.md','a') as f:\n",
    "        if 'content' in dic.keys():\n",
    "            f.write(str(base64.b64decode(dic['content']).decode('UTF8')))\n",
    "\n",
    "def data_retrieve():    \n",
    "    search_condition = '?q=framework+language:javascript+stars:>1&sort=starts&order=dec'\n",
    "    prefix = 'https://api.github.com/search/repositories'\n",
    "    r1 = requests.get(prefix+search_condition)\n",
    "    t1 = r1.text\n",
    "    dict1 = json.loads(t1)\n",
    "    print('number of retrieved repos:'+'\\t'+str(dict1['total_count']))\n",
    "\n",
    "    repos_number = dict1['total_count'] if dict1['total_count'] < 50 else 50\n",
    "    i = 1\n",
    "    for repo in dict1['items']:\n",
    "        retrieve_readme(repo['url'],str(i))\n",
    "        print(repo['url'])\n",
    "        i = i + 1 \n",
    "        time.sleep(1)\n",
    "        if i > repos_number:\n",
    "            break\n",
    "            \n",
    "def documents_iter(directory='./Readme'):\n",
    "    for file in os.listdir(path=directory):\n",
    "        if file[0:4]!='read':\n",
    "            continue\n",
    "        with open(directory+'/'+file,'r') as f:\n",
    "            yield f\n",
    "tw = ['\\t','\\n']\n",
    "def stem_documents(documents_arr):\n",
    "    lancaster = stem.lancaster.LancasterStemmer()\n",
    "    def mystem (word):\n",
    "            word = word.lower()\n",
    "            if word not in tw:\n",
    "                sword = lancaster.stem(word)\n",
    "            else:\n",
    "                sword = word\n",
    "            return (sword)\n",
    "    def mytokenize (doc):\n",
    "        def mycleanfile_line(l):\n",
    "            l = re.sub(r'http[\\S]+[\\s]',r' ',l)\n",
    "            l = re.sub(r'[#\\*=_]+',r' ',l)\n",
    "            l = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@\\[\\]^_`{|}~]',r' ',l)\n",
    "            l = re.sub(r'[\\s]+',r' ',l)\n",
    "            return l\n",
    "        doc = ''.join([mycleanfile_line(line) for line in doc])\n",
    "        return (word_tokenize(doc))\n",
    "    texts = [[mystem(word) for word in mytokenize(document) if (word not in stopwords.words('english')) and (mystem(word) not in stopwords.words('english'))] for document in documents_arr()]\n",
    "    #texts = [[mystem(word) for word in list(mytokenize(document)) if (word not in stopwords) and (mystem(word) not in stopwords)] for document in documents_arr()]\n",
    "#all_tokens = sum (texts, []) \n",
    "#    tokens_once = set (word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "#    texts = [[word for word in text if word not in tokens_once] for text in texts]\n",
    "    #return texts\n",
    "    return texts\n",
    "\n",
    "    \n",
    "\n",
    "def write_clean_data():\n",
    "    with open('Readme_source.md','a') as f1:\n",
    "        for file in stem_documents(documents_iter):\n",
    "            for i in file:\n",
    "                f1.write(i+' ')\n",
    "            f1.write('\\n')\n",
    "\n",
    "model_word2vec = models.Word2Vec(stem_documents(documents_iter), min_count=10)\n",
    "print(model_word2vec.vocab.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
